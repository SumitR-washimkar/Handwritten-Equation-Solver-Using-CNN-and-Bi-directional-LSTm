{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "098bcde4-3c36-4c01-96f1-9d1b140770a4",
   "metadata": {},
   "source": [
    "## Dataset and Main Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a37fcb1-9c86-4fba-865e-e76aecb18b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c68d7989-de0f-4972-9b53-00502e540d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6de2697e-aecc-4224-abf7-eaaf9d227328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column1</th>\n",
       "      <th>Column2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18_em_0.bmp</td>\n",
       "      <td>x _ { k } x x _ { k } + y _ { k } y x _ { k }</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18_em_10.bmp</td>\n",
       "      <td>2 6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18_em_11.bmp</td>\n",
       "      <td>q _ { t } = 2 q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18_em_12.bmp</td>\n",
       "      <td>\\frac { p e ^ { t } } { 1 - ( 1 - p ) e ^ { t } }</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18_em_13.bmp</td>\n",
       "      <td>4 ^ { 2 } + 4 ^ { 2 } + \\frac { 4 } { 4 }</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Column1                                            Column2\n",
       "0   18_em_0.bmp      x _ { k } x x _ { k } + y _ { k } y x _ { k }\n",
       "1  18_em_10.bmp                                                2 6\n",
       "2  18_em_11.bmp                                    q _ { t } = 2 q\n",
       "3  18_em_12.bmp  \\frac { p e ^ { t } } { 1 - ( 1 - p ) e ^ { t } }\n",
       "4  18_em_13.bmp          4 ^ { 2 } + 4 ^ { 2 } + \\frac { 4 } { 4 }"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path='../Handwritten_equations/Handwritten/Dataset'\n",
    "df=pd.read_csv('caption_data.csv')\n",
    "\n",
    "# Modyfing the name of images\n",
    "df[\"Column1\"] = df[\"Column1\"].astype(str) + \".bmp\"\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed798d2-1caa-40d8-b92f-5e8560c2bfb2",
   "metadata": {},
   "source": [
    "## Trainning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22fbc873-e723-4a8b-aadc-84498c24b121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, tokenizer, device, clip_grad_norm=1.0):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass with teacher forcing\n",
    "        outputs = model(images, labels, teacher_forcing_ratio=0.5)\n",
    "\n",
    "        # Assumptions about SOS token id from tokenizer\n",
    "        sos_token_id = tokenizer.token_to_id.get(\"<SOS>\", 1)\n",
    "        pad_token_id = tokenizer.token_to_id.get(\"<PAD>\", 0)\n",
    "\n",
    "        # Shift outputs and labels to ignore SOS token prediction in loss\n",
    "        outputs_shifted = outputs[:, 1:, :].contiguous()\n",
    "        labels_shifted = labels[:, 1:].contiguous()\n",
    "\n",
    "        # Create mask to ignore loss on padding tokens\n",
    "        mask = labels_shifted != pad_token_id  # [B, seq_len]\n",
    "\n",
    "        # Flatten outputs and labels for loss computation\n",
    "        outputs_flat = outputs_shifted.view(-1, tokenizer.vocab_size())\n",
    "        labels_flat = labels_shifted.view(-1)\n",
    "\n",
    "        # Apply mask to ignore padding tokens in loss\n",
    "        mask_flat = mask.view(-1)\n",
    "        outputs_flat = outputs_flat[mask_flat]\n",
    "        labels_flat = labels_flat[mask_flat]\n",
    "\n",
    "        # Compute loss only on non-padded tokens\n",
    "        loss = criterion(outputs_flat, labels_flat)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader) if len(dataloader) > 0 else 0.0\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13c7138-ec8d-4c82-aea9-ac6f8b749351",
   "metadata": {},
   "source": [
    "## Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28b128e4-311c-462f-88c2-e7f1d403dfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion, tokenizer, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass without teacher forcing (autoregressive generation)\n",
    "            outputs = model(images, targets=None, teacher_forcing_ratio=0.0)\n",
    "\n",
    "            # Token IDs\n",
    "            sos_token_id = tokenizer.token_to_id.get(\"<SOS>\", 1)\n",
    "            pad_token_id = tokenizer.token_to_id.get(\"<PAD>\", 0)\n",
    "\n",
    "            # Shift outputs and labels to ignore SOS token\n",
    "            outputs_shifted = outputs[:, 1:, :].contiguous()\n",
    "            labels_shifted = labels[:, 1:].contiguous()\n",
    "\n",
    "            # Mask padding tokens in labels for loss calculation\n",
    "            mask = labels_shifted != pad_token_id  # [B, seq_len]\n",
    "\n",
    "            # Flatten outputs and labels for loss\n",
    "            outputs_flat = outputs_shifted.view(-1, tokenizer.vocab_size())\n",
    "            labels_flat = labels_shifted.view(-1)\n",
    "            mask_flat = mask.view(-1)\n",
    "\n",
    "            # Apply mask to outputs and labels\n",
    "            outputs_flat = outputs_flat[mask_flat]\n",
    "            labels_flat = labels_flat[mask_flat]\n",
    "\n",
    "            # Compute loss only on non-padded tokens\n",
    "            if labels_flat.numel() == 0:\n",
    "                # Skip batch if no valid labels\n",
    "                continue\n",
    "\n",
    "            loss = criterion(outputs_flat, labels_flat)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader) if len(dataloader) > 0 else 0.0\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691315ed-46ea-4a96-aa21-4a5a619ab033",
   "metadata": {},
   "source": [
    "## Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afb025ba-799d-4543-a5f4-57dc6ef98a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, image, tokenizer, device, max_len=50):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        image = image.to(device).unsqueeze(0)\n",
    "        encoder_out = model.encoder(image)\n",
    "\n",
    "        sos_token_id = tokenizer.token_to_id.get(\"<SOS>\", 1)\n",
    "        eos_token_id = tokenizer.token_to_id.get(\"<EOS>\", 2)\n",
    "\n",
    "        inputs = torch.tensor([sos_token_id]).to(device)\n",
    "\n",
    "        # Initialize hidden state from encoder output\n",
    "        encoder_mean = encoder_out.mean(dim=1)  # [1, H]\n",
    "        h_0 = encoder_mean.unsqueeze(0)         # [1, 1, H]\n",
    "        c_0 = torch.zeros_like(h_0)             # [1, 1, H]\n",
    "        hidden = (h_0, c_0)\n",
    "\n",
    "        decoded_tokens = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            output, hidden, _ = model.decoder(inputs, hidden, encoder_out)\n",
    "            top1 = output.argmax(1)\n",
    "\n",
    "            if top1.item() == eos_token_id:\n",
    "                break\n",
    "\n",
    "            decoded_tokens.append(top1.item())\n",
    "            inputs = top1\n",
    "\n",
    "        return tokenizer.decode(decoded_tokens) if decoded_tokens else \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521d84a9-f64b-411d-b294-dc65f48e3dd1",
   "metadata": {},
   "source": [
    "## Use of Tokenizer and Creating Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "414a2a95-ffc2-481f-a74a-1d2c379d5d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "from Modules.Tokenizer import Tokenizer\n",
    "from Modules.EquationSeqDataset import EquationSeqDataset\n",
    "\n",
    "\n",
    "# 1. Tokenizer improvements\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit(df['Column2'])\n",
    "\n",
    "# Consider adding special tokens explicitly if your tokenizer supports it:\n",
    "special_tokens = [\"<PAD>\", \"<SOS>\", \"<EOS>\", \"<UNK>\"]\n",
    "for token in special_tokens:\n",
    "    if token not in tokenizer.token_to_id:\n",
    "        tokenizer.add_token(token)  # or the equivalent tokenizer method\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),               # Slightly larger for random crop\n",
    "    transforms.RandomHorizontalFlip(p=0.5),      # Flip with 50% chance\n",
    "    transforms.RandomRotation(15),                # Smaller rotation range (more realistic)\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(                         # Normalize with ImageNet stats (if using ResNet pretrained)\n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "# 3. Instantiate dataset with improved transforms\n",
    "dataset = EquationSeqDataset(df, path, tokenizer, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b086680-61cb-4edd-9c45-bba4bb392c1e",
   "metadata": {},
   "source": [
    "## Custom Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6a3f9df-5d2c-41b7-a005-3374fe8ef673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "from Modules.custom_collate_fn import custom_collate_fn\n",
    "\n",
    "# 1. Fix random seed for reproducibility and stable split\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()\n",
    "\n",
    "# 2. Stratified split (optional, if your labels have classes)\n",
    "# If your dataset has class imbalance, consider stratified splitting\n",
    "# via sklearn's StratifiedShuffleSplit or custom logic.\n",
    "\n",
    "# 3. Use a validation set instead of test split for hyperparameter tuning\n",
    "train_ratio = 0.8\n",
    "train_size = int(train_ratio * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_data, test_data = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    collate_fn=custom_collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    collate_fn=custom_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c8693d-b3cb-4c10-931d-dda8d257769d",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9a6ee05-1244-4fda-958a-439dc5748fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Windows\\Anaconda\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Windows\\Anaconda\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "from Modules.Encoder import CNNEncoder\n",
    "from Modules.Decoder import RNNDecoder\n",
    "from Modules.Sequence import Seq2Seq\n",
    "from Modules.EarlyStopping import EarlyStopping\n",
    "\n",
    "encoder = CNNEncoder(output_dim=256).to(device)\n",
    "decoder = RNNDecoder(hidden_dim=256, vocab_size=tokenizer.vocab_size()).to(device)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "# Use CrossEntropyLoss ignoring PAD token (assumed 0)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.token_to_id.get(\"<PAD>\", 0)).to(device)\n",
    "\n",
    "# AdamW optimizer with weight decay helps generalization\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=4, min_delta=0.001, save_path='best_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962406d1-bbba-417f-98d1-e4f22c7c1126",
   "metadata": {},
   "source": [
    "## Trainning Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad06c3ff-7f47-4134-85b0-a4a7a5429763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sumit Washimkar SRW\\AppData\\Local\\Temp\\ipykernel_12140\\1899001269.py:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch import amp\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "epochs = 50\n",
    "train_loss_values = []\n",
    "val_loss_values = []\n",
    "\n",
    "# Use automatic mixed precision for faster training & better generalization\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "    # --- Training ---\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        images, labels = batch[0], batch[1]\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        with amp.autocast(device_type='cuda'):\n",
    "            outputs = model(images, labels, teacher_forcing_ratio=0.5)\n",
    "            # Prepare targets and outputs for loss (same as in your train function)\n",
    "            start_idx = tokenizer.token_to_id.get(\"<SOS>\", 1)\n",
    "            outputs_shifted = outputs[:, start_idx:, :].contiguous()\n",
    "            labels_shifted = labels[:, start_idx:]\n",
    "            outputs_flat = outputs_shifted.reshape(-1, tokenizer.vocab_size())\n",
    "            labels_flat = labels_shifted.reshape(-1)\n",
    "            loss = criterion(outputs_flat, labels_flat)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_loss_values.append(avg_train_loss)\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    early_stopping(avg_train_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered. Loading best model weights...\")\n",
    "        model.load_state_dict(torch.load(early_stopping.save_path))\n",
    "        break\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"Epoch Time: {end - start:.2f} seconds\\n\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4834757-e281-4fa2-b600-c5e364bece03",
   "metadata": {},
   "source": [
    "## Prediction on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce736dce-7859-47c9-8218-837de7c71ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in train_loader:\n",
    "        images, labels = batch[0], batch[1]\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        batch_size = images.size(0)\n",
    "\n",
    "        # Predict batch-wise (avoid loop over single images when possible)\n",
    "        for i in range(batch_size):\n",
    "            image = images[i]\n",
    "            label = labels[i]\n",
    "\n",
    "            # Extract true sequence between <SOS> and <EOS>\n",
    "            try:\n",
    "                eos_indices = (label == tokenizer.token_to_id[\"<EOS>\"]).nonzero(as_tuple=True)[0]\n",
    "                eos_index = eos_indices[0].item() if eos_indices.numel() > 0 else (label != 0).sum().item()\n",
    "            except IndexError:  # No EOS token found\n",
    "                eos_index = (label != 0).sum().item()  # Use length ignoring padding\n",
    "            \n",
    "            start_idx = tokenizer.token_to_id.get(\"<SOS>\", 1)\n",
    "            true_tokens = label[start_idx:eos_index]\n",
    "            true_latex = tokenizer.decode(true_tokens.cpu().tolist())  # Move to CPU for decode\n",
    "\n",
    "            predicted_latex = predict(model, image.cpu(), tokenizer, device)  # Move image to CPU if predict expects CPU input\n",
    "\n",
    "            similarity = SequenceMatcher(None, predicted_latex.strip(), true_latex.strip()).ratio()\n",
    "\n",
    "            # Adjust threshold or use more sophisticated metrics (BLEU, ROUGE, etc.)\n",
    "            if similarity >= 0.60:  # Slightly stricter threshold to reduce false positives\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "accuracy = (correct / total) * 100 if total > 0 else 0.0\n",
    "print(f\"Approximate Match Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1173619e-4283-4991-859d-a867f9772b8c",
   "metadata": {},
   "source": [
    "## Prediction on Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd4d1e7-c1d7-40f5-a91e-574e8238444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in train_loader:\n",
    "        images, labels = batch[0], batch[1]\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        batch_size = images.size(0)\n",
    "\n",
    "        # Predict batch-wise (avoid loop over single images when possible)\n",
    "        for i in range(batch_size):\n",
    "            image = images[i]\n",
    "            label = labels[i]\n",
    "\n",
    "            # Extract true sequence between <SOS> and <EOS>\n",
    "            try:\n",
    "                eos_indices = (label == tokenizer.token_to_id[\"<EOS>\"]).nonzero(as_tuple=True)[0]\n",
    "                eos_index = eos_indices[0].item() if eos_indices.numel() > 0 else (label != 0).sum().item()\n",
    "            except IndexError:  # No EOS token found\n",
    "                eos_index = (label != 0).sum().item()  # Use length ignoring padding\n",
    "            \n",
    "            start_idx = tokenizer.token_to_id.get(\"<SOS>\", 1)\n",
    "            true_tokens = label[start_idx:eos_index]\n",
    "            true_latex = tokenizer.decode(true_tokens.cpu().tolist())  # Move to CPU for decode\n",
    "\n",
    "            predicted_latex = predict(model, image.cpu(), tokenizer, device)  # Move image to CPU if predict expects CPU input\n",
    "\n",
    "            similarity = SequenceMatcher(None, predicted_latex.strip(), true_latex.strip()).ratio()\n",
    "\n",
    "            # Adjust threshold or use more sophisticated metrics (BLEU, ROUGE, etc.)\n",
    "            if similarity >= 0.60:  # Slightly stricter threshold to reduce false positives\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "accuracy = (correct / total) * 100 if total > 0 else 0.0\n",
    "print(f\"Approximate Match Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3431b6-fb68-4f7d-a910-b71c151b316b",
   "metadata": {},
   "source": [
    "## Visulisation of Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156c5c67-4508-4af2-9077-b5378c49cb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "epoch=50\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, epoch + 1), train_loss_values, linestyle='-', color='b', label=\"Training Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training & Validation Loss vs. Epoch\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddd8dc4-5a6a-41ab-984e-94f4ce01c52c",
   "metadata": {},
   "source": [
    "## Visulisation of Some images and their Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa3d83e-4422-4b56-b81a-6fd3fca419c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = 0.005\n",
    "check1_size = int(check * len(dataset))\n",
    "check2_size = len(dataset) - check1_size\n",
    "check1, check2 = random_split(dataset, [check1_size, check2_size])\n",
    "\n",
    "check_loader = DataLoader(\n",
    "    check1,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    collate_fn=custom_collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563ea57a-6809-417d-8cc9-05d60c7461be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in check_loader:\n",
    "        images, labels = batch[0], batch[1]\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        for i in range(len(images)):\n",
    "            image = images[i].unsqueeze(0)  # Add batch dimension\n",
    "            label = labels[i]\n",
    "\n",
    "            # Get <SOS> and <EOS> indices\n",
    "            sos_idx = tokenizer.token_to_id.get(\"<SOS>\", 1)\n",
    "            eos_idx = tokenizer.token_to_id.get(\"<EOS>\", 2)\n",
    "\n",
    "            # Find <EOS> index if present\n",
    "            eos_pos = (label == eos_idx).nonzero(as_tuple=True)\n",
    "            eos_indices = (label == tokenizer.token_to_id[\"<EOS>\"]).nonzero(as_tuple=True)[0]\n",
    "            eos_index = eos_indices[0].item() if eos_indices.numel() > 0 else (label != 0).sum().item()\n",
    "\n",
    "            # Extract ground-truth token sequence\n",
    "            true_tokens = label[sos_idx:eos_index]\n",
    "            true_latex = tokenizer.decode(true_tokens.cpu().tolist())\n",
    "\n",
    "            # Predict LaTeX from model\n",
    "            predicted_latex = predict(model, image.squeeze(0), tokenizer, device)\n",
    "\n",
    "            # Log output\n",
    "            print(f\"True Tokens        : {true_tokens.cpu().tolist()}\")\n",
    "            print(f\"Ground Truth LaTeX : {true_latex}\")\n",
    "            print(f\"Predicted LaTeX    : {predicted_latex}\")\n",
    "            print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fef7bca-6a55-426b-b276-2f1f2b3e63e7",
   "metadata": {},
   "source": [
    "## Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018578ab-42bf-4311-a032-e1cdf6e09ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "\n",
    "# Define special token IDs\n",
    "sos_token_id = tokenizer.token_to_id.get(\"<SOS>\", 1)\n",
    "eos_token_id = tokenizer.token_to_id.get(\"<EOS>\", 2)\n",
    "\n",
    "# Save model checkpoint\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'sos_token_id': sos_token_id,\n",
    "    'eos_token_id': eos_token_id\n",
    "}, 'model_checkpoint.pth')\n",
    "\n",
    "# Save tokenizer\n",
    "with open('tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "print(\"Model and tokenizer saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b2a73e-5b1a-40a8-ad1b-803771aa97ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97365804-c70a-4aaf-90f5-8128984e7d3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
