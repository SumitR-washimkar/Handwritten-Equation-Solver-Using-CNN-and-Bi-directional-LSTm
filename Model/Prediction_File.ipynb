{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "058e9795-71e4-46ba-922f-d8b4f20511a3",
   "metadata": {},
   "source": [
    "# Importing The Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e09d596d-57bd-4c58-bc85-98b68d90534d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sumit Washimkar\n",
      "[nltk_data]     SRW\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Windows\\Anaconda\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Windows\\Anaconda\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): CNNEncoder(\n",
       "    (feature_extractor): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (4): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (fc): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (decoder): RNNDecoder(\n",
       "    (embedding): Embedding(95, 256)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "    (attn): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (v): Linear(in_features=256, out_features=1, bias=False)\n",
       "    (lstm): LSTM(512, 256, batch_first=True)\n",
       "    (fc_out): Linear(in_features=256, out_features=95, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "\n",
    "from Modules.Encoder import CNNEncoder\n",
    "from Modules.Decoder import RNNDecoder\n",
    "from Modules.Sequence import Seq2Seq\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load tokenizer\n",
    "with open('tokenizer.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "# Model setup (must match training setup)\n",
    "encoder = CNNEncoder(output_dim=256).to(device)\n",
    "decoder = RNNDecoder(hidden_dim=256, vocab_size=tokenizer.vocab_size()).to(device)\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load('model_checkpoint.pth', map_location=device)\n",
    "\n",
    "sos_token_id = checkpoint['sos_token_id']\n",
    "eos_token_id = checkpoint['eos_token_id']\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, sos_token_id, eos_token_id, device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab207c15-05c9-4201-96d6-fb34e67d8afa",
   "metadata": {},
   "source": [
    "# Demonstraction of Image and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c075bf4-3b2b-41a9-8517-2fc127d2fa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "def predict_single_image(model, tokenizer, image_path, device, transform, max_len=50):\n",
    "    model.eval()\n",
    "\n",
    "    # Load and preprocess image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0).to(device)  # [1, C, H, W]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_out = model.encoder(image)\n",
    "\n",
    "        # Get token IDs\n",
    "        sos_token_id = tokenizer.token_to_id.get(\"<SOS>\", 1)\n",
    "        eos_token_id = tokenizer.token_to_id.get(\"<EOS>\", 2)\n",
    "\n",
    "        # Init decoder\n",
    "        inputs = torch.tensor([sos_token_id]).to(device)\n",
    "\n",
    "        # Init hidden from encoder output\n",
    "        encoder_mean = encoder_out.mean(dim=1)  # [1, H]\n",
    "        h_0 = encoder_mean.unsqueeze(0)         # [1, 1, H]\n",
    "        c_0 = torch.zeros_like(h_0)             # [1, 1, H]\n",
    "        hidden = (h_0, c_0)\n",
    "\n",
    "        decoded_tokens = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            output, hidden, _ = model.decoder(inputs, hidden, encoder_out)\n",
    "            top1 = output.argmax(1)\n",
    "\n",
    "            if top1.item() == eos_token_id:\n",
    "                break\n",
    "\n",
    "            decoded_tokens.append(top1.item())\n",
    "            inputs = top1\n",
    "\n",
    "        return tokenizer.decode(decoded_tokens) if decoded_tokens else \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "622f2984-ab54-42a5-b33d-2b3e28b7f17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),               # Slightly larger for random crop\n",
    "    transforms.RandomHorizontalFlip(p=0.5),      # Flip with 50% chance\n",
    "    transforms.RandomRotation(15),                # Smaller rotation range (more realistic)\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(                         # Normalize with ImageNet stats (if using ResNet pretrained)\n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c92a95ec-974b-4204-b091-6d29190bed9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted LaTeX: ( f ( a ) ) = = a a b b\n"
     ]
    }
   ],
   "source": [
    "image_path1 = \"18_em_6.bmp\"  # Path to your image\n",
    "predicted_latex = predict_single_image(model, tokenizer, image_path1, device, transform)\n",
    "print(\"Predicted LaTeX:\", predicted_latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "364efd4e-a26b-457b-b21f-48b26acd2df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted LaTeX: [ [ n n ] ] ] ] ] ] ] ] ]\n"
     ]
    }
   ],
   "source": [
    "image_path2 = \"Demo1.png\"  # Path to your image\n",
    "predicted_latex = predict_single_image(model, tokenizer, image_path2, device, transform)\n",
    "print(\"Predicted LaTeX:\", predicted_latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0234e64b-71fe-4cf3-9b6c-952ae8cbad78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted LaTeX: [ [ [ [ m m m m m m m\n"
     ]
    }
   ],
   "source": [
    "image_path3 = \"Demo2.png\"  # Path to your image\n",
    "predicted_latex = predict_single_image(model, tokenizer, image_path3, device, transform)\n",
    "print(\"Predicted LaTeX:\", predicted_latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca911cc4-5f90-4d46-8daa-f3f7f68cea2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
